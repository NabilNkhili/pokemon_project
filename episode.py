import requests
from bs4 import BeautifulSoup
import rdflib
import re
import time

BASE_URL = "https://bulbapedia.bulbagarden.net"
EPISODES_URL = BASE_URL + "/wiki/List_of_animated_series_episodes"

def fetch_episode_links():
    """R√©cup√®re tous les liens vers les pages d'√©pisodes en filtrant les balises invalides avec gestion des erreurs."""
    retries = 3  # Nombre de tentatives en cas d'√©chec
    episode_links = []

    for attempt in range(retries):
        try:
            response = requests.get(EPISODES_URL, timeout=10)
            if response.status_code != 200:
                print(f"‚ùå Erreur {response.status_code} lors du chargement de la page : {EPISODES_URL}")
                continue  # R√©essaye la requ√™te

            soup = BeautifulSoup(response.text, 'html.parser')

            # ‚úÖ Extraction s√©curis√©e des liens
            for link in soup.select('table a'):
                try:
                    episode_name = link.text.strip()
                    episode_url = BASE_URL + link['href']
                    # Filtrer les liens internes et les cat√©gories
                    if not "Category:" in link['href'] and episode_name:
                        episode_links.append((episode_name, episode_url))
                except KeyError:
                    print(f"‚ö†Ô∏è Balise ignor√©e, absence de 'href' : {link}")

            if episode_links:
                print(f"‚úÖ {len(episode_links)} √©pisodes trouv√©s.")
                return episode_links

        except requests.exceptions.RequestException as e:
            print(f"‚ö†Ô∏è Erreur r√©seau ({attempt+1}/{retries}) pour la page des √©pisodes : {e}")
            time.sleep(5)  # Attente avant nouvelle tentative

    print("‚ùå √âchec final de la r√©cup√©ration des √©pisodes.")
    return []

def fetch_episode_details(episode_url):
    """Extrait les d√©tails de l'√©pisode avec gestion des erreurs et d√©lai entre les requ√™tes."""
    retries = 3
    for attempt in range(retries):
        try:
            response = requests.get(episode_url, timeout=10)
            time.sleep(0.5)  # Petite pause pour √©viter le blocage IP

            if response.status_code != 200:
                print(f"‚ùå Erreur {response.status_code} pour {episode_url}")
                continue  # R√©essaye

            soup = BeautifulSoup(response.text, 'html.parser')
            infobox_table = soup.find('table', {'class': 'roundy'})
            if not infobox_table:
                print(f"‚ùå Aucune infobox trouv√©e pour : {episode_url}")
                return {}

            episode_data = {}

            # ‚úÖ Extraction des champs de l'infobox
            for row in infobox_table.find_all('tr'):
                header = row.find('th')
                value = row.find('td')
                if header and value:
                    key = header.get_text(strip=True).lower().replace(' ', '_')
                    value_text = value.get_text(strip=True).replace('\n', ', ')
                    episode_data[key] = value_text

            # ‚úÖ Extraction de l'image
            image_element = infobox_table.find('img')
            if image_element and 'src' in image_element.attrs:
                image_url = image_element['src']
                if image_url.startswith("//"):
                    image_url = "https:" + image_url
                elif not image_url.startswith("http"):
                    image_url = "https://archives.bulbagarden.net" + image_url
                episode_data['image'] = image_url

            print(f"‚úÖ Donn√©es extraites depuis : {episode_url}")
            return episode_data

        except requests.exceptions.RequestException as e:
            print(f"‚ö†Ô∏è Erreur r√©seau ({attempt+1}/{retries}) pour {episode_url}: {e}")
            time.sleep(5)  # Attente avant nouvelle tentative

    print(f"‚ùå √âchec final pour {episode_url}, on passe au suivant.")
    return {}

def generate_rdf_graph(episode_list):
    """G√©n√®re un fichier RDF Turtle pour tous les √©pisodes."""
    EX = rdflib.Namespace("http://example.org/pokemon/episodes/")
    g = rdflib.Graph()
    g.bind('ex', EX)

    total_episodes = 0

    for episode_name, episode_url in episode_list:
        print(f"üîç Extraction des donn√©es pour : {episode_name}")
        episode_entity = rdflib.URIRef(EX + re.sub(r'[^a-zA-Z0-9_]', '_', episode_name))
        g.add((episode_entity, rdflib.RDF.type, EX.Episode))
        g.add((episode_entity, rdflib.RDFS.label, rdflib.Literal(episode_name)))

        episode_details = fetch_episode_details(episode_url)
        if episode_details:
            for key, value in episode_details.items():
                g.add((episode_entity, EX[key], rdflib.Literal(value)))
            total_episodes += 1
        else:
            print(f"‚ö†Ô∏è Aucune donn√©e ajout√©e pour : {episode_name}")

        # ‚úÖ Sauvegarde toutes les 50 entr√©es pour √©viter la perte en cas d'arr√™t
        if total_episodes % 50 == 0:
            g.serialize("pokemon_all_episodes.ttl", format="turtle", encoding="utf-8")
            print(f"üíæ Sauvegarde interm√©diaire apr√®s {total_episodes} √©pisodes.")

    # ‚úÖ Sauvegarde finale
    g.serialize("pokemon_all_episodes.ttl", format="turtle", encoding="utf-8")
    print(f"‚úÖ Fichier RDF g√©n√©r√© avec succ√®s : pokemon_all_episodes.ttl")
    print(f"üé¨ Nombre total d'√©pisodes trait√©s : {total_episodes}")

# ‚úÖ Ex√©cution principale : R√©cup√®re tous les √©pisodes et g√©n√®re le RDF
episode_links = fetch_episode_links()
if episode_links:
    generate_rdf_graph(episode_links)
else:
    print("‚ùå Aucune donn√©e trouv√©e pour g√©n√©rer le RDF.")
