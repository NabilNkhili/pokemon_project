import requests
from bs4 import BeautifulSoup
import rdflib
import re
import time
from datetime import datetime

BASE_URL = "https://bulbapedia.bulbagarden.net"
EPISODES_URL = BASE_URL + "/wiki/List_of_animated_series_episodes"

# Namespaces
EX = rdflib.Namespace("http://example.org/pokemon/")
EP = rdflib.Namespace("http://example.org/episodes/")

def fetch_episode_links():
    """R√©cup√®re les liens vers toutes les pages d'√©pisodes."""
    retries = 3  # Nombre de tentatives en cas d'√©chec
    episode_links = []

    for attempt in range(retries):
        try:
            response = requests.get(EPISODES_URL, timeout=10)
            if response.status_code != 200:
                print(f"‚ùå Erreur {response.status_code} lors du chargement de la page : {EPISODES_URL}")
                continue  # R√©essaye la requ√™te

            soup = BeautifulSoup(response.text, 'html.parser')

            # Extraction s√©curis√©e des liens
            for link in soup.select('table a'):
                try:
                    episode_name = link.text.strip()
                    episode_url = BASE_URL + link['href']
                    # Filtrer les liens internes et les cat√©gories
                    if not "Category:" in link['href'] and episode_name:
                        episode_links.append((episode_name, episode_url))
                except KeyError:
                    print(f"‚ö†Ô∏è Balise ignor√©e, absence de 'href' : {link}")

            if episode_links:
                print(f"‚úÖ {len(episode_links)} √©pisodes trouv√©s.")
                return episode_links

        except requests.exceptions.RequestException as e:
            print(f"‚ö†Ô∏è Erreur r√©seau ({attempt+1}/{retries}) pour la page des √©pisodes : {e}")
            time.sleep(5)  # Attente avant nouvelle tentative

    print("‚ùå √âchec final de la r√©cup√©ration des √©pisodes.")
    return []

def fetch_episode_details(episode_url):
    """Extrait les d√©tails de l'√©pisode, y compris les Pok√©mon d√©buts."""
    retries = 3
    for attempt in range(retries):
        try:
            response = requests.get(episode_url, timeout=10)
            time.sleep(0.5)  # Petite pause pour √©viter le blocage IP

            if response.status_code != 200:
                print(f"‚ùå Erreur {response.status_code} pour {episode_url}")
                continue  # R√©essaye

            soup = BeautifulSoup(response.text, 'html.parser')
            infobox_table = soup.find('table', {'class': 'roundy'})
            if not infobox_table:
                print(f"‚ùå Aucune infobox trouv√©e pour : {episode_url}")
                return {}

            episode_data = {}

            # Extraction des champs de l'infobox
            for row in infobox_table.find_all('tr'):
                header = row.find('th')
                value = row.find('td')
                if header and value:
                    key = header.get_text(strip=True).lower().replace(' ', '_')
                    value_text = value.get_text(strip=True).replace('\n', ', ')
                    episode_data[key] = value_text

            # Extraction de l'image
            image_element = infobox_table.find('img')
            if image_element and 'src' in image_element.attrs:
                image_url = image_element['src']
                if image_url.startswith("//"):
                    image_url = "https:" + image_url
                elif not image_url.startswith("http"):
                    image_url = "https://archives.bulbagarden.net" + image_url
                episode_data['image'] = image_url

            # Extraction des Pok√©mon d√©buts
            pokemon_debuts = []
            for heading in soup.find_all('span', {'class': 'mw-headline'}):
                if "Pok√©mon debuts" in heading.text:
                    list_items = heading.find_next('ul').find_all('li')
                    for item in list_items:
                        pokemon_name = item.text.strip()
                        pokemon_name_cleaned = re.sub(r'\([^)]*\)', '', pokemon_name).strip()
                        if pokemon_name_cleaned:
                            pokemon_debuts.append(pokemon_name_cleaned)
                    break

            episode_data['pokemon_debuts'] = pokemon_debuts

            print(f"‚úÖ Donn√©es extraites depuis : {episode_url}")
            return episode_data

        except requests.exceptions.RequestException as e:
            print(f"‚ö†Ô∏è Erreur r√©seau ({attempt+1}/{retries}) pour {episode_url}: {e}")
            time.sleep(5)  # Attente avant nouvelle tentative

    print(f"‚ùå √âchec final pour {episode_url}, on passe au suivant.")
    return {}

def generate_rdf_graph(episode_list):
    """G√©n√®re un fichier RDF Turtle pour tous les √©pisodes."""
    g = rdflib.Graph()
    g.bind('ex', EX)
    g.bind('ep', EP)

    total_episodes = 0

    for episode_name, episode_url in episode_list:
        print(f"üîç Traitement de l'√©pisode {episode_name} ({total_episodes + 1}/{len(episode_list)})")
        episode_entity = rdflib.URIRef(EP + re.sub(r'[^a-zA-Z0-9_]', '_', episode_name))
        g.add((episode_entity, rdflib.RDF.type, EP.Episode))
        g.add((episode_entity, EP.hasTitle, rdflib.Literal(episode_name)))

        episode_details = fetch_episode_details(episode_url)
        if episode_details:
            # Ajout des propri√©t√©s de l'√©pisode
            g.add((episode_entity, EP.hasEpisodeNumber, rdflib.Literal(episode_details.get('episode_number', ''))))
            
            # Gestion des dates
            for date_key, date_prop in [('japan', EP.hasJapanReleaseDate), ('united_states', EP.hasUSReleaseDate)]:
                date_str = episode_details.get(date_key, '')
                try:
                    date_obj = datetime.strptime(date_str, "%B %d, %Y")  # Adaptez le format si n√©cessaire
                    g.add((episode_entity, date_prop, rdflib.Literal(date_obj, datatype=rdflib.XSD.date)))
                except ValueError:
                    print(f"‚ö†Ô∏è Format de date invalide pour {episode_name}: {date_str}")

            g.add((episode_entity, EP.hasImage, rdflib.Literal(episode_details.get('image', ''))))
            g.add((episode_entity, EP.hasAnimation, rdflib.Literal(episode_details.get('animation', ''))))
            g.add((episode_entity, EP.hasDirector, rdflib.Literal(episode_details.get('animation_directors', ''))))
            g.add((episode_entity, EP.hasScreenplay, rdflib.Literal(episode_details.get('screenplay', ''))))
            g.add((episode_entity, EP.hasStoryboard, rdflib.Literal(episode_details.get('storyboard', ''))))
            g.add((episode_entity, EP.hasOpening, rdflib.Literal(episode_details.get('opening', ''))))
            g.add((episode_entity, EP.hasEnding, rdflib.Literal(episode_details.get('ending', ''))))

            # Ajout des Pok√©mon d√©buts
            for pokemon_name in episode_details.get('pokemon_debuts', []):
                pokemon_entity = rdflib.URIRef(EX + re.sub(r'[^a-zA-Z0-9_]', '_', pokemon_name))
                g.add((episode_entity, EP.hasPokemonDebut, pokemon_entity))

            total_episodes += 1
        else:
            print(f"‚ö†Ô∏è Aucune donn√©e ajout√©e pour : {episode_name}")

        # Sauvegarde interm√©diaire
        if total_episodes % 50 == 0:
            g.serialize("pokemon_all_episodes.ttl", format="turtle", encoding="utf-8")
            print(f"üíæ Sauvegarde interm√©diaire apr√®s {total_episodes} √©pisodes.")

    # Sauvegarde finale
    g.serialize("pokemon_all_episodes.ttl", format="turtle", encoding="utf-8")
    print(f"‚úÖ Fichier RDF g√©n√©r√© avec succ√®s : pokemon_all_episodes.ttl")
    print(f"üé¨ Nombre total d'√©pisodes trait√©s : {total_episodes}")

# Ex√©cution principale
episode_links = fetch_episode_links()
if episode_links:
    generate_rdf_graph(episode_links)
else:
    print("‚ùå Aucune donn√©e trouv√©e pour g√©n√©rer le RDF.")